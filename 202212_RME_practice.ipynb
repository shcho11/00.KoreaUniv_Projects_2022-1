{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJQ7pZNFBsh8XWV7zlqcWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shcho11/00.Projects_KoreaUniv_2022-1/blob/main/202212_RME_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMtKS1fNu-AJ",
        "outputId": "2f238ea1-cd3b-4733-d517-b989cf219965"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P80FD_RCTf1Z",
        "outputId": "8f9e3c91-72fa-45a6-8d50-ccd97ae8e87f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.16 (default, Dec  7 2022, 01:12:13) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w4siNHQy55ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install produce_negative_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acs18yd-5aD7",
        "outputId": "822b8f94-a7ee-490f-b681-cf8b6e6b4b25"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement produce_negative_embedding (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for produce_negative_embedding\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install text_util"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPMv9vqVzB2c",
        "outputId": "2e84f250-5074-486d-e548-38cb402a8a9f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting text_util\n",
            "  Downloading text_util-0.1.1-py3-none-any.whl (1.7 kB)\n",
            "Requirement already satisfied: click<8.0.0,>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from text_util) (7.1.2)\n",
            "Collecting clipboard-util<0.2.0,>=0.1.1\n",
            "  Downloading clipboard_util-0.1.2-py3-none-any.whl (2.1 kB)\n",
            "Collecting pyperclip<2.0.0,>=1.8.0\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=84077251396c958d2c60d3fd972e55627ac294a73172f2a50d3d6a4c85a7d3a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, clipboard-util, text-util\n",
            "Successfully installed clipboard-util-0.1.2 pyperclip-1.8.2 text-util-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-text-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88yknbNXUEHc",
        "outputId": "f300be09-29a9-4fa1-9c11-419718f95743"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-text-utils\n",
            "  Downloading python_text_utils-0.0.6-py2.py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from python-text-utils) (1.15.0)\n",
            "Installing collected packages: python-text-utils\n",
            "Successfully installed python-text-utils-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install constants"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNuIE1KPcdD7",
        "outputId": "b5af2b9c-0d64-4a1a-de12-f1e89173a0ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting constants\n",
            "  Downloading constants-0.6.0.tar.gz (5.1 kB)\n",
            "Building wheels for collected packages: constants\n",
            "  Building wheel for constants (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for constants: filename=constants-0.6.0-py3-none-any.whl size=5473 sha256=897d9fab29d3556f35160672cb51edf07d9d6bb53aae633180bae69b688da31e\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/ac/b2/89268490b92bf6fd0102b3634668042437e0e024c64ef447a1\n",
            "Successfully built constants\n",
            "Installing collected packages: constants\n",
            "Successfully installed constants-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wmf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zv_sbYLeQRB",
        "outputId": "812d7d4d-3189-4055-d435-6d684f9a9531"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wmf\n",
            "  Downloading wmf-0.4.2.tar.gz (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 3.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wmf\n",
            "  Building wheel for wmf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wmf: filename=wmf-0.4.2-cp38-cp38-linux_x86_64.whl size=585605 sha256=52c67a0611f9354392373ba21da7c19875a05e927364ac7529a59b2b97351ec2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/39/3d/8eceaf95902c25ab77105ef03a69fb755af6192b682f26a2f0\n",
            "Successfully built wmf\n",
            "Installing collected packages: wmf\n",
            "Successfully installed wmf-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOtG9tLMegzt",
        "outputId": "1ed9676d-447e-4579-cc10-7e1f7e401f23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyll\n",
            "  Downloading Pyll-0.4.3.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 3.9 MB/s \n",
            "\u001b[?25hCollecting Paste>=1.7.5.1\n",
            "  Using cached Paste-3.5.2-py2.py3-none-any.whl (593 kB)\n",
            "Collecting PasteScript>=1.7.5\n",
            "  Using cached PasteScript-3.2.1-py2.py3-none-any.whl (73 kB)\n",
            "Collecting Beaker>=1.6.4\n",
            "  Downloading Beaker-1.12.0.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting FormEncode>=1.2.4\n",
            "  Downloading FormEncode-2.0.1-py2.py3-none-any.whl (363 kB)\n",
            "\u001b[K     |████████████████████████████████| 363 kB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.6 in /usr/local/lib/python3.8/dist-packages (from pyll) (2.11.3)\n",
            "Collecting Genshi>=0.6\n",
            "  Downloading Genshi-0.7.7-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 14.6 MB/s \n",
            "\u001b[?25hCollecting WebError>=0.10.3\n",
            "  Downloading WebError-0.13.1.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting WebHelpers>=1.3\n",
            "  Downloading WebHelpers-1.3.tar.gz (729 kB)\n",
            "\u001b[K     |████████████████████████████████| 729 kB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psycopg2>=2.4 in /usr/local/lib/python3.8/dist-packages (from pyll) (2.9.5)\n",
            "Collecting transaction>=1.3\n",
            "  Downloading transaction-3.0.1-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting zope.sqlalchemy>=0.7.1\n",
            "  Downloading zope.sqlalchemy-1.6-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: Pygments>=1.5 in /usr/local/lib/python3.8/dist-packages (from pyll) (2.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.15 in /usr/local/lib/python3.8/dist-packages (from pyll) (2.0.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from pyll) (4.4.2)\n",
            "Requirement already satisfied: SQLAlchemy>=0.7.9 in /usr/local/lib/python3.8/dist-packages (from pyll) (1.4.44)\n",
            "Collecting WebOb>=0.5.1\n",
            "  Downloading WebOb-1.8.7-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from FormEncode>=1.2.4->pyll) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from Paste>=1.7.5.1->pyll) (57.4.0)\n",
            "Collecting PasteDeploy\n",
            "  Using cached PasteDeploy-3.0.1-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy>=0.7.9->pyll) (2.0.1)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (261 kB)\n",
            "\u001b[K     |████████████████████████████████| 261 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting Tempita\n",
            "  Downloading Tempita-0.5.2-py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: pyll, Beaker, WebError, WebHelpers\n",
            "  Building wheel for pyll (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyll: filename=Pyll-0.4.3-py3-none-any.whl size=1188720 sha256=82dc348d617300e97d35b0d012a3963e1299d1d06d892b4d22a29c90d8470ec5\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/da/d1/e7caf67fc14158a6fbc43f5e6d822516162bb5efbffb0ed6ce\n",
            "  Building wheel for Beaker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Beaker: filename=Beaker-1.12.0-py3-none-any.whl size=51440 sha256=5d8808ac5db65e869594218a82a2ef4390d14ed720ed520beb0cb459ad2ed4bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/e2/06/b45fb00cf10607a47e1e4be803a748a958fd1bb558d0bead60\n",
            "  Building wheel for WebError (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for WebError: filename=WebError-0.13.1-py3-none-any.whl size=89449 sha256=f817b2d1942352e3f2f1059f051f8769bd6c4837444589f18331fdd587800213\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/3f/39/9b98ffb5e39b4325d4404a1b92a29649563da7f80907076fb8\n",
            "  Building wheel for WebHelpers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for WebHelpers: filename=WebHelpers-1.3-py3-none-any.whl size=149051 sha256=05b55e58c7db7922a59f56f38a68bffe15a9c4cdd0be4d5804f78fb6197d1cb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/03/48/7bb543e415b4fc845d555dbb517180570a1671caa3c861afbf\n",
            "Successfully built pyll Beaker WebError WebHelpers\n",
            "Installing collected packages: zope.interface, WebOb, transaction, Tempita, PasteDeploy, Paste, zope.sqlalchemy, WebHelpers, WebError, PasteScript, Genshi, FormEncode, Beaker, pyll\n",
            "Successfully installed Beaker-1.12.0 FormEncode-2.0.1 Genshi-0.7.7 Paste-3.5.2 PasteDeploy-3.0.1 PasteScript-3.2.1 Tempita-0.5.2 WebError-0.13.1 WebHelpers-1.3 WebOb-1.8.7 pyll-0.4.3 transaction-3.0.1 zope.interface-5.5.2 zope.sqlalchemy-1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install produce_negative_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4scgWkteZE-",
        "outputId": "2eb9ffdc-58aa-46f6-b53a-e1f8376e8df8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package produce_negative_embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "import numpy as np\n",
        "import time\n",
        "import python_text_utils\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.utils import shuffle\n",
        "from joblib import Parallel, delayed\n",
        "import constants as gc\n",
        "\n",
        "np.random.seed(98765) #set random seed\n",
        "\n",
        "import argparse"
      ],
      "metadata": {
        "id": "dxjzh0OvyyNj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 머신러닝 모델의 하이퍼 파라미터를 쉽게 관리할 수 있다\n",
        "# Ref : https://supermemi.tistory.com/69\n",
        "\n",
        "# 인자값을 받을 수 있는 인스턴스 생성\n",
        "parser = argparse.ArgumentParser(\"Description: Running multi-embedding recommendation - RME model\")\n",
        "\n",
        "parser.add_argument('-f')\n",
        "\n",
        "# 입력받을 인자값 설정 (default 값 설정가능)\n",
        "parser.add_argument('--data_path', default='data', type=str, help='path to the data')\n",
        "parser.add_argument('--batch_size', default=5000, type=float, help='batch processing')\n",
        "parser.add_argument('--dataset', default=\"ml10m\", type=str, help='dataset')\n",
        "\n",
        "# args 에 위의 내용 저장. \n",
        "args = parser.parse_args()\n",
        "\n",
        "# 입력받은 인자값 출력\n",
        "\n",
        "print(args.data_path)\n",
        "print(args.batch_size)\n",
        "print(args.dataset)"
      ],
      "metadata": {
        "id": "988k3ZY8yygW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3a2de3-a086-44cd-8903-1f478ad1f710"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\n",
            "5000\n",
            "ml10m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tpWW7z55yymD",
        "outputId": "09618cf0-7bf4-4235-e883-13239314518c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.path.join : \n",
        "\"\"\"\n",
        "경로(패스)명 조작에 관한 처리를 모아둔 모듈로써 구현되어 있는 함수의 하나이다. \n",
        "인수에 전달된 2개의 문자열을 결합하여, 1개의 경로로 할 수 있다.\n",
        " os.path.jon()을 사용하기 위해서는, os 모듈을 import할 필요가 있다.\n",
        "\"\"\"\n",
        "# Ref : https://engineer-mole.tistory.com/188 \n",
        "\n",
        "DATA_DIR =  os.path.join('drive', 'MyDrive', 'dl2022')\n",
        "\n",
        "unique_uid = list()\n",
        "with open(os.path.join(DATA_DIR, 'unique_uid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_uid.append(line.strip())\n",
        "\n",
        "\n",
        "unique_movieId = list()\n",
        "with open(os.path.join(DATA_DIR, 'unique_sid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_movieId.append(line.strip())\n",
        "\n",
        "n_items = len(unique_movieId)\n",
        "n_users = len(unique_uid)\n",
        "\n",
        "print('number of users:%d ,  number of items: %d'%(n_users, n_items))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIzTSCHXyyjA",
        "outputId": "e8e7dce6-4dd0-4602-d22a-2bdb61442bbd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of users:20279 ,  number of items: 6051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(csv_file, shape=(n_users, n_items)):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    rows, cols = np.array(tp['userId']), np.array(tp['movieId']) \n",
        "    #rows will be user ids, cols will be item-ids. that is.. user-item matrix form\n",
        "    seq = np.concatenate((  rows[:, None], cols[:, None], np.ones((rows.size, 1), dtype='int')\n",
        "                          ), axis=1)\n",
        "    data = sparse.csr_matrix((np.ones_like(rows), (rows, cols)), dtype=np.int16, shape=shape)\n",
        "    return data, seq, tp"
      ],
      "metadata": {
        "id": "8ffprJvWyyo_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate item-item co-occurrence matrix based on the user's consumed items history ############\n",
        "### This will build a item-item co-occurrence matrix           ############################\n",
        "### user 1: item 1, item 2, ... item k --> item 1, 2, ..., k will be seen as a sentence ==> do co-occurrence.\n",
        "\n",
        "def _coord_batch(lo, hi, train_data, prefix = 'item', max_neighbor_words = 100000, choose='macro'):\n",
        "    rows = []\n",
        "    cols = []\n",
        "\n",
        "    for u in range(lo, hi):\n",
        "      # shcho: at the original codes, the author used \"in xrange\" form which does not work in python3 environment.\n",
        "        #print train_data[u].nonzero()[1] #names all the item ids that the user at index u watched nonzero return a\n",
        "        # 2D array, index 0 will be the row index and index 1 will be columns whose values are not equal to 0\n",
        "        lst_words = train_data[u].nonzero()[1]\n",
        "        if len(lst_words) > max_neighbor_words:\n",
        "            \n",
        "            if choose == 'micro':\n",
        "                #approach 1: randomly select max_neighbor_words for each word.\n",
        "                for w in lst_words:\n",
        "                    tmp = lst_words.remove(w)\n",
        "                    #random choose max_neigbor words in the list:\n",
        "                    neighbors = np.random.choice(tmp, max_neighbor_words, replace=False)\n",
        "                    for c in neighbors:\n",
        "                        rows.append(w)\n",
        "                        cols.append(c)\n",
        "            \n",
        "            if choose == 'macro':\n",
        "                #approach 2: randomly select the sentence with length of max_neigbor_words + 1, then do permutation.\n",
        "                lst_words = np.random.choice(lst_words, max_neighbor_words + 1, replace=False)\n",
        "                for w, c in itertools.permutations(lst_words, 2):\n",
        "                    rows.append(w)\n",
        "                    cols.append(c)\n",
        "        else:\n",
        "            for w, c in itertools.permutations(lst_words, 2):\n",
        "                rows.append(w)\n",
        "                cols.append(c)\n",
        "    if not os.path.exists(os.path.join(DATA_DIR, 'co-temp')): os.mkdir(os.path.join(DATA_DIR, 'co-temp'))\n",
        "    np.save(os.path.join(DATA_DIR, 'co-temp' ,'%s_coo_%d_%d.npy' % (prefix, lo, hi)),\n",
        "            np.concatenate([np.array(rows)[:, None], np.array(cols)[:, None]], axis=1)) #append column wise.\n",
        "    pass\n",
        "\n",
        "batch_size = args.batch_size\n",
        "\n",
        "train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train.csv'))\n",
        "\n",
        "#clear the co-temp folder:\n",
        "if os.path.exists(os.path.join(DATA_DIR, 'co-temp')):\n",
        "    for f in glob.glob(os.path.join(DATA_DIR, 'co-temp', '*.npy')):\n",
        "        os.remove(f)\n",
        "\n",
        "GENERATE_ITEM_ITEM_COOCCURENCE_FILE = True\n",
        "if GENERATE_ITEM_ITEM_COOCCURENCE_FILE:\n",
        "    t1 = time.time()\n",
        "    print ('Generating item item co-occurrence matrix')\n",
        "    start_idx = range(0, n_users, batch_size)\n",
        "    # shcho : convert 'start_idx''s type from range to list\n",
        "    list_start_idx = list(start_idx)\n",
        "    end_idx = list_start_idx[1:] + [n_users]\n",
        "    Parallel(n_jobs=1)(delayed(_coord_batch)(lo, hi, train_data, prefix = 'item', max_neighbor_words = 200) for lo, hi in zip(start_idx, end_idx))\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds'%(t2-t1))\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5S9nC2r5KQf",
        "outputId": "ed344b30-2019-440e-a36d-e13a967cdb73"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating item item co-occurrence matrix\n",
            "Time : 49 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(start_idx)\n",
        "print(list_start_idx)\n",
        "print(end_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs7UG55ib4XN",
        "outputId": "4d0a80c5-f5d8-445e-a70a-747082c30f3b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "range(0, 20279, 5000)\n",
            "[0, 5000, 10000, 15000, 20000]\n",
            "[5000, 10000, 15000, 20000, 20279]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shcho : define save_pickle, load_pickle\n",
        "import pickle\n",
        "\n",
        "# save\n",
        "def save_pickle(data, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
        "        print ('Saved %s..' %path)\n",
        "\n",
        "# load\n",
        "def load_pickle(path):\n",
        "   with open(path, 'rb') as f:\n",
        "      pickle.load(f)"
      ],
      "metadata": {
        "id": "bZNL1psDWYW-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################\n",
        "### Generate user-user co-occurrence matrix based on the same items they backed################\n",
        "### This will build a user-user co-occurrence matrix ##########################################\n",
        "\n",
        "GENERATE_USER_USER_COOCCURENCE_FILE = True\n",
        "if GENERATE_USER_USER_COOCCURENCE_FILE:\n",
        "    t1 = time.time()\n",
        "    print('Generating user user co-occurrence matrix')\n",
        "    start_idx = range(0, n_items, batch_size)\n",
        "    list_start_idx = list(start_idx)\n",
        "    end_idx = list_start_idx[1:] + [n_items]\n",
        "    Parallel(n_jobs=8)(delayed(_coord_batch)(lo, hi, train_data.T, prefix = 'user', max_neighbor_words=100) for lo, hi in zip(start_idx, end_idx))\n",
        "    t2 = time.time()\n",
        "    print('Time : %d seconds'%(t2 - t1))\n",
        "    pass\n",
        "\n",
        "################################################################################################\n",
        "\n",
        "def _load_coord_matrix(start_idx, end_idx, nrow, ncol, prefix = 'item'):\n",
        "    X = sparse.csr_matrix((nrow, ncol), dtype='float32')\n",
        "\n",
        "    for lo, hi in zip(start_idx, end_idx):\n",
        "        coords = np.load(os.path.join(DATA_DIR, 'co-temp', '%s_coo_%d_%d.npy' % (prefix, lo, hi)))\n",
        "\n",
        "        rows = coords[:, 0]\n",
        "        cols = coords[:, 1]\n",
        "\n",
        "        tmp = sparse.coo_matrix((np.ones_like(rows), (rows, cols)), shape=(nrow, ncol), dtype='float32').tocsr()\n",
        "        X = X + tmp\n",
        "\n",
        "        print(\"%s %d to %d finished\" % (prefix, lo, hi))\n",
        "        sys.stdout.flush()\n",
        "    return X\n",
        "\n",
        "BOOLEAN_LOAD_PP_COOCC_FROM_FILE = True\n",
        "X, Y = None, None\n",
        "if BOOLEAN_LOAD_PP_COOCC_FROM_FILE:\n",
        "    print ('Loading item item co-occurrence matrix')\n",
        "    t1 = time.time()\n",
        "    start_idx = range(0, n_users, batch_size)\n",
        "    list_start_idx = list(start_idx)\n",
        "    end_idx = list_start_idx[1:] + [n_users]\n",
        "    X = _load_coord_matrix(start_idx, end_idx, n_items, n_items, prefix = 'item') #item item co-occurrence matrix\n",
        "    print ('dumping matrix ...')\n",
        "    save_pickle(X, os.path.join(DATA_DIR,'item_item_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds'%(t2-t1))\n",
        "else:\n",
        "    print ('test loading model from pickle file')\n",
        "    t1 = time.time()\n",
        "    X = load_pickle(os.path.join(DATA_DIR,'item_item_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('[INFO]: sparse matrix size of item-item co-occurrence matrix: %d mb\\n' % (\n",
        "                                                    (X.data.nbytes + X.indices.nbytes + X.indptr.nbytes) / (1024 * 1024)))\n",
        "    print ('Time : %d seconds'%(t2-t1))\n",
        "\n",
        "#X = None\n",
        "BOOLEAN_LOAD_UU_COOCC_FROM_FILE = True\n",
        "if BOOLEAN_LOAD_UU_COOCC_FROM_FILE:\n",
        "    print ('Loading user user co-occurrence matrix')\n",
        "    t1 = time.time()\n",
        "    start_idx = range(0, n_items, batch_size)\n",
        "    list_start_idx = list(start_idx)\n",
        "    end_idx = list_start_idx[1:] + [n_items]\n",
        "    Y = _load_coord_matrix(start_idx, end_idx, n_users, n_users, prefix = 'user') #user user co-occurrence matrix\n",
        "\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "    print ('dumping matrix ...')\n",
        "    t1 = time.time()\n",
        "    save_pickle(Y, os.path.join(DATA_DIR, 'user_user_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds'%(t2-t1))\n",
        "else:\n",
        "    print ('test loading model from pickle file')\n",
        "    t1 = time.time()\n",
        "    Y = load_pickle(os.path.join(DATA_DIR, 'user_user_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('[INFO]: sparse matrix size of user user co-occurrence matrix: %d mb\\n' % (\n",
        "                                                    (Y.data.nbytes + Y.indices.nbytes + Y.indptr.nbytes) / (1024 * 1024)))\n",
        "    print ('Time : %d seconds'%(t2-t1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEdnQEl35KTH",
        "outputId": "8b86a084-7f4d-483e-bd7d-99779fe3ae2e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating user user co-occurrence matrix\n",
            "Time : 16 seconds\n",
            "Loading item item co-occurrence matrix\n",
            "item 0 to 5000 finished\n",
            "item 5000 to 10000 finished\n",
            "item 10000 to 15000 finished\n",
            "item 15000 to 20000 finished\n",
            "item 20000 to 20279 finished\n",
            "dumping matrix ...\n",
            "Saved drive/MyDrive/dl2022/item_item_cooc.dat..\n",
            "Time : 13 seconds\n",
            "Loading user user co-occurrence matrix\n",
            "user 0 to 5000 finished\n",
            "user 5000 to 6051 finished\n",
            "Time : 1 seconds\n",
            "dumping matrix ...\n",
            "Saved drive/MyDrive/dl2022/user_user_cooc.dat..\n",
            "Time : 0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Q0cvhree3j",
        "outputId": "37fd28a4-0435-420d-a752-0ac9389cdb15"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1)\t349.0\n",
            "  (0, 2)\t278.0\n",
            "  (0, 3)\t42.0\n",
            "  (0, 4)\t231.0\n",
            "  (0, 5)\t712.0\n",
            "  (0, 6)\t341.0\n",
            "  (0, 7)\t23.0\n",
            "  (0, 8)\t74.0\n",
            "  (0, 9)\t514.0\n",
            "  (0, 10)\t506.0\n",
            "  (0, 11)\t36.0\n",
            "  (0, 12)\t40.0\n",
            "  (0, 13)\t161.0\n",
            "  (0, 14)\t32.0\n",
            "  (0, 15)\t342.0\n",
            "  (0, 16)\t775.0\n",
            "  (0, 17)\t106.0\n",
            "  (0, 18)\t99.0\n",
            "  (0, 19)\t27.0\n",
            "  (0, 20)\t653.0\n",
            "  (0, 21)\t184.0\n",
            "  (0, 22)\t52.0\n",
            "  (0, 23)\t129.0\n",
            "  (0, 24)\t683.0\n",
            "  (0, 25)\t105.0\n",
            "  :\t:\n",
            "  (6050, 5860)\t1.0\n",
            "  (6050, 5877)\t2.0\n",
            "  (6050, 5911)\t1.0\n",
            "  (6050, 5915)\t1.0\n",
            "  (6050, 5918)\t1.0\n",
            "  (6050, 5919)\t1.0\n",
            "  (6050, 5940)\t1.0\n",
            "  (6050, 5942)\t1.0\n",
            "  (6050, 5950)\t1.0\n",
            "  (6050, 5959)\t1.0\n",
            "  (6050, 5967)\t2.0\n",
            "  (6050, 5972)\t1.0\n",
            "  (6050, 5975)\t1.0\n",
            "  (6050, 5984)\t1.0\n",
            "  (6050, 5989)\t1.0\n",
            "  (6050, 5996)\t2.0\n",
            "  (6050, 6002)\t1.0\n",
            "  (6050, 6019)\t1.0\n",
            "  (6050, 6021)\t1.0\n",
            "  (6050, 6028)\t1.0\n",
            "  (6050, 6038)\t1.0\n",
            "  (6050, 6040)\t1.0\n",
            "  (6050, 6043)\t1.0\n",
            "  (6050, 6046)\t1.0\n",
            "  (6050, 6048)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A3rbA5ken98",
        "outputId": "cd23d25a-da15-432f-95c5-02be145b53d9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 150)\t1.0\n",
            "  (0, 267)\t1.0\n",
            "  (0, 412)\t1.0\n",
            "  (0, 448)\t1.0\n",
            "  (0, 499)\t1.0\n",
            "  (0, 547)\t1.0\n",
            "  (0, 725)\t1.0\n",
            "  (0, 1157)\t1.0\n",
            "  (0, 1224)\t1.0\n",
            "  (0, 1332)\t1.0\n",
            "  (0, 1379)\t1.0\n",
            "  (0, 1419)\t1.0\n",
            "  (0, 1585)\t1.0\n",
            "  (0, 1966)\t1.0\n",
            "  (0, 2124)\t1.0\n",
            "  (0, 2809)\t1.0\n",
            "  (0, 2867)\t1.0\n",
            "  (0, 3374)\t1.0\n",
            "  (0, 3383)\t1.0\n",
            "  (0, 3409)\t1.0\n",
            "  (0, 3425)\t1.0\n",
            "  (0, 3427)\t1.0\n",
            "  (0, 3577)\t1.0\n",
            "  (0, 3600)\t1.0\n",
            "  (0, 3662)\t1.0\n",
            "  :\t:\n",
            "  (20278, 20016)\t1.0\n",
            "  (20278, 20023)\t1.0\n",
            "  (20278, 20028)\t1.0\n",
            "  (20278, 20031)\t2.0\n",
            "  (20278, 20053)\t1.0\n",
            "  (20278, 20059)\t3.0\n",
            "  (20278, 20063)\t1.0\n",
            "  (20278, 20071)\t1.0\n",
            "  (20278, 20090)\t2.0\n",
            "  (20278, 20106)\t1.0\n",
            "  (20278, 20117)\t1.0\n",
            "  (20278, 20120)\t2.0\n",
            "  (20278, 20121)\t1.0\n",
            "  (20278, 20125)\t1.0\n",
            "  (20278, 20135)\t1.0\n",
            "  (20278, 20156)\t3.0\n",
            "  (20278, 20188)\t1.0\n",
            "  (20278, 20193)\t1.0\n",
            "  (20278, 20214)\t1.0\n",
            "  (20278, 20216)\t1.0\n",
            "  (20278, 20236)\t1.0\n",
            "  (20278, 20255)\t1.0\n",
            "  (20278, 20272)\t1.0\n",
            "  (20278, 20273)\t1.0\n",
            "  (20278, 20276)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate item-item co-occurrence matrix based on the user backed items history ########\n",
        "### This will build a item-item co-occurrence matrix           ############################\n",
        "### user 1: item 1, item 2, ... item k --> item 1, 2, ..., k will be seen as a sentence ==> do co-occurrence.\n",
        "\n",
        "def _coord_batch_n(lo, hi, train_data, prefix = 'item', max_neighbor_words = 200, choose='macro'):\n",
        "    rows = []\n",
        "    cols = []\n",
        "\n",
        "    for u in range(lo, hi):\n",
        "\n",
        "        lst_words = train_data[u].nonzero()[1]\n",
        "\n",
        "        if len(lst_words) > max_neighbor_words:\n",
        "            if choose == 'micro':\n",
        "                #approach 1: randomly select max_neighbor_words for each word.\n",
        "                for w in lst_words:\n",
        "                    tmp = lst_words.remove(w)\n",
        "                    #random choose max_neigbor words in the list:\n",
        "                    neighbors = np.random.choice(tmp, max_neighbor_words, replace=False)\n",
        "                    for c in neighbors:\n",
        "                        rows.append(w)\n",
        "                        cols.append(c)\n",
        "            if choose == 'macro':\n",
        "                #approach 2: randomly select the sentence with length of max_neigbor_words + 1, then do permutation.\n",
        "                lst_words = np.random.choice(lst_words, max_neighbor_words + 1, replace=False)\n",
        "                for w, c in itertools.permutations(lst_words, 2):\n",
        "                    rows.append(w)\n",
        "                    cols.append(c)\n",
        "        else:\n",
        "\n",
        "            for w, c in itertools.permutations(lst_words, 2):\n",
        "              # Ref : https://docs.python.org/ko/3/library/itertools.html\n",
        "                rows.append(w)\n",
        "                cols.append(c)\n",
        "    if not os.path.exists(os.path.join(DATA_DIR, 'negative-co-temp')): os.mkdir(os.path.join(DATA_DIR, 'negative-co-temp'))\n",
        "    np.save(os.path.join(DATA_DIR, 'negative-co-temp' ,'negative_%s_coo_%d_%d.npy' % (prefix, lo, hi)),\n",
        "            np.concatenate([np.array(rows)[:, None], np.array(cols)[:, None]], axis=1)) #append column wise.\n",
        "    pass\n",
        "\n",
        "batch_size = args.batch_size\n",
        "\n",
        "train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train_neg.csv'))\n",
        "#clear the negative-co-temp folder:\n",
        "if os.path.exists(os.path.join(DATA_DIR, 'negative-co-temp')):\n",
        "    for f in glob.glob(os.path.join(DATA_DIR, 'negative-co-temp', '*.npy')):\n",
        "        os.remove(f)\n",
        "\n",
        "t1 = time.time()\n",
        "print ('Generating item item negative_co-occurrence matrix')\n",
        "start_idx = range(0, n_users, batch_size)\n",
        "list_start_idx = list(start_idx)\n",
        "end_idx = list_start_idx[1:] + [n_users]\n",
        "Parallel(n_jobs=1)(delayed(_coord_batch_n)(lo, hi, train_data, prefix = 'item') for lo, hi in zip(start_idx, end_idx))\n",
        "t2 = time.time()\n",
        "print ('Time : %d seconds'%(t2-t1))\n",
        "pass\n",
        "\n",
        "\n",
        "def _load_coord_matrix(start_idx, end_idx, nrow, ncol, prefix = 'item'):\n",
        "    X = sparse.csr_matrix((nrow, ncol), dtype='float32')\n",
        "\n",
        "    for lo, hi in zip(start_idx, end_idx):\n",
        "        coords = np.load(os.path.join(DATA_DIR, 'negative-co-temp', 'negative_%s_coo_%d_%d.npy' % (prefix, lo, hi)))\n",
        "\n",
        "        rows = coords[:, 0]\n",
        "        cols = coords[:, 1]\n",
        "\n",
        "        tmp = sparse.coo_matrix((np.ones_like(rows), (rows, cols)), shape=(nrow, ncol), dtype='float32').tocsr()\n",
        "        X = X + tmp\n",
        "\n",
        "        print(\"%s %d to %d finished\" % (prefix, lo, hi))\n",
        "        sys.stdout.flush()\n",
        "    return X\n",
        "\n",
        "\n",
        "X, Y = None, None\n",
        "print ('Loading item item negative_co-occurrence matrix and saving to pickle file for fast loading')\n",
        "t1 = time.time()\n",
        "start_idx = range(0, n_users, batch_size)\n",
        "list_start_idx = list(start_idx)\n",
        "end_idx = list_start_idx[1:] + [n_users]\n",
        "X = _load_coord_matrix(start_idx, end_idx, n_items, n_items, prefix = 'item') #item item co-occurrence matrix\n",
        "print ('dumping matrix ...')\n",
        "save_pickle(X, os.path.join(DATA_DIR,'negative_item_item_cooc.dat'))\n",
        "t2 = time.time()\n",
        "print ('Time : %d seconds'%(t2-t1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Z4MgUm5KVd",
        "outputId": "ab9a98ee-6309-43f0-9f47-ad1dba3b1b2d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating item item negative_co-occurrence matrix\n",
            "Time : 6 seconds\n",
            "Loading item item negative_co-occurrence matrix and saving to pickle file for fast loading\n",
            "item 0 to 5000 finished\n",
            "item 5000 to 10000 finished\n",
            "item 10000 to 15000 finished\n",
            "item 15000 to 20000 finished\n",
            "item 20000 to 20279 finished\n",
            "dumping matrix ...\n",
            "Saved drive/MyDrive/dl2022/negative_item_item_cooc.dat..\n",
            "Time : 0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgz5EXREqUFi",
        "outputId": "d5dbcf6d-2808-4b21-f1b2-ae3a457690c8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py-modelrunner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybcn2zCFl-XQ",
        "outputId": "a9a5ee24-a5b1-462b-f57e-7267485e4dcc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting py-modelrunner\n",
            "  Downloading py_modelrunner-0.10.0-py3-none-any.whl (38 kB)\n",
            "Collecting h5py>=3.5\n",
            "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from py-modelrunner) (1.21.6)\n",
            "Requirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.8/dist-packages (from py-modelrunner) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.45 in /usr/local/lib/python3.8/dist-packages (from py-modelrunner) (4.64.1)\n",
            "Collecting jinja2>=3\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.8/dist-packages (from py-modelrunner) (1.3.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2>=3->py-modelrunner) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3->py-modelrunner) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3->py-modelrunner) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3->py-modelrunner) (1.15.0)\n",
            "Installing collected packages: jinja2, h5py, py-modelrunner\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "notebook 5.7.16 requires jinja2<=3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-3.7.0 jinja2-3.1.2 py-modelrunner-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Parallel, delayed\n",
        "import glob\n",
        "import os\n",
        "import modelrunner as ModelRunner # https://pypi.org/project/py-modelrunner/\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import global_constants as gc\n",
        "import time\n",
        "import text_utils\n",
        "import pandas as pd\n",
        "import argparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "2_H9dStvl2uX",
        "outputId": "9a256fca-a805-4f14-897b-456362fdbcaa"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-3aed2b761b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mglobal_constants\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'global_constants'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(\"Description: Running multi-embedding recommendation - RME model\")\n",
        "parser.add_argument('-f')\n",
        "\n",
        "#parser.add_argument('--data_path', default='data', type=str, help='path to the data')\n",
        "parser.add_argument('--saved_model_path', default='MODELS', type=str, help='path to save the optimal learned parameters')\n",
        "parser.add_argument('--s', default=1, type=int, help='a pre-defined shifted value for measuring SPPMI')\n",
        "parser.add_argument('--model', default='rme', type=str, help='the model to run: rme, cofactor')\n",
        "parser.add_argument('--n_factors', default=40, type=int, help='number of hidden factors for user/item representation')\n",
        "parser.add_argument('--reg', default=1.0, type=float, help='regularization for user and item latent factors (alpha, beta)')\n",
        "parser.add_argument('--reg_embed', default=1.0, type=float, help='regularization for user and item context latent factors (gamma, delta, theta)')\n",
        "#parser.add_argument('--dataset', default=\"ml10m\", type=str, help='dataset')\n",
        "parser.add_argument('--neg_item_inference', default=0, type=int, help='if there is no available disliked items, set this to 1 to infer '\n",
        "                                                                      'negative items for users using our user-oriented EM like algorithm')\n",
        "parser.add_argument('--neg_sample_ratio', default=0.2, type=float, help='negative sample ratio per user. If a user consumed 10 items, and this'\n",
        "                                                                        'neg_sample_ratio = 0.2 --> randomly sample 2 negative items for the user')\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "vM6xjxvo5KYC"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(args.saved_model_path)\n",
        "print(args.s)\n",
        "print(args.model)\n",
        "print(args.n_factors)\n",
        "print(args.reg)\n",
        "print(args.reg_embed)\n",
        "print(args.neg_item_inference)\n",
        "print(args.neg_sample_ratio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mAod72XkNw4",
        "outputId": "c3783eb6-a920-4c1c-f756-9cf369250574"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODELS\n",
            "1\n",
            "rme\n",
            "40\n",
            "1.0\n",
            "1.0\n",
            "0\n",
            "0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR =  os.path.join('drive', 'MyDrive', 'dl2022')\n",
        "# gc.DATA_DIR = DATA_DIR\n",
        "# gc.SAVED_MODLE_DIR = args.saved_model_path\n",
        "# gc.PRED_DIR = os.path.join(DATA_DIR, 'prediction-temp')\n",
        "#SHIFTED_K_VALUE = args.s\n",
        "#NEGATIVE_SAMPLE_RATIO = args.neg_sample_ratio\n",
        "#save_dir = os.path.join(DATA_DIR, 'model_tmp_res')\n",
        "#n_components = args.n_factors\n",
        "#lam = args.reg\n",
        "#lam_emb = args.reg_embed\n",
        "\n",
        "unique_uid = list()\n",
        "with open(os.path.join(DATA_DIR, 'unique_uid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_uid.append(line.strip())\n",
        "\n",
        "unique_movieId = list()\n",
        "with open(os.path.join(DATA_DIR, 'unique_sid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_movieId.append(line.strip())\n",
        "\n",
        "n_items = len(unique_movieId)\n",
        "n_users = len(unique_uid)\n",
        "n_items = len(unique_movieId)\n",
        "print(n_users, n_items)\n",
        "\n",
        "def load_data(csv_file, shape=(n_users, n_items)):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    rows, cols = np.array(tp['userId']), np.array(tp['movieId'])\n",
        "    seq = np.concatenate((rows[:, None], cols[:, None], np.ones((rows.size, 1), dtype='int')), axis=1)\n",
        "    data = sparse.csr_matrix((np.ones_like(rows), (rows, cols)), dtype=np.int16, shape=shape)\n",
        "    return data, seq, tp\n",
        "\n",
        "def get_row(M, i):\n",
        "    # get the row i of sparse matrix:\n",
        "    lo, hi = M.indptr[i], M.indptr[i + 1]\n",
        "    return lo, hi, M.data[lo:hi], M.indices[lo:hi]\n",
        "\n",
        "\n",
        "def convert_to_SPPMI_matrix(M, max_row, shifted_K=1):\n",
        "    # if we sum the co-occurrence matrix by row wise or column wise --> we have an array that contain the #(i) values\n",
        "    obj_counts = np.asarray(M.sum(axis=1)).ravel()\n",
        "    total_obj_pairs = M.data.sum()\n",
        "    M_sppmi = M.copy()\n",
        "    for i in xrange(max_row):\n",
        "        lo, hi, data, indices = get_row(M, i)\n",
        "        M_sppmi.data[lo:hi] = np.log(data * total_obj_pairs / (obj_counts[i] * obj_counts[indices]))\n",
        "    M_sppmi.data[M_sppmi.data < 0] = 0\n",
        "    M_sppmi.eliminate_zeros()\n",
        "    if shifted_K == 1:\n",
        "        return M_sppmi\n",
        "    else:\n",
        "        M_sppmi.data -= np.log(shifted_K)\n",
        "        M_sppmi.data[M_sppmi.data < 0] = 0\n",
        "        M_sppmi.eliminate_zeros()\n",
        "    return M_sppmi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek9FRi7q5Kao",
        "outputId": "00408d48-cf97-4727-e8e8-0a1a1cc0d9d3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20279 6051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wmf\n",
        "import pyll\n",
        "from scipy import sparse\n",
        "import glob\n",
        "import os\n",
        "# import produce_negative_embedding as pne # produce_negative_embedding 이 패키지가 없음..."
      ],
      "metadata": {
        "id": "a1glt4i2eGNy"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.neg_item_inference:\n",
        "    #initialize with WMF:\n",
        "    import wmf\n",
        "#    import rec_eval\n",
        "    from scipy import sparse\n",
        "#    import produce_negative_embedding as pne\n",
        "    import glob\n",
        "    import os\n",
        "\n",
        "    def softmax(x):\n",
        "        \"\"\"Compute softmax values for each ranked list.\"\"\"\n",
        "        # We want the item with higher ranking score have lower prob to be withdrawn as negative instances #\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def compute_neg_prob(ranks):\n",
        "        return softmax(np.negative(ranks))\n",
        "\n",
        "    def _make_prediction(train_data, Et, Eb, user_idx, batch_users, mu=None,\n",
        "                         vad_data=None):\n",
        "        n_songs = train_data.shape[1]\n",
        "        # exclude examples from training and validation (if any)\n",
        "        item_idx = np.zeros((batch_users, n_songs), dtype=bool)\n",
        "        item_idx[train_data[user_idx].nonzero()] = True\n",
        "        if vad_data is not None:\n",
        "            item_idx[vad_data[user_idx].nonzero()] = True\n",
        "        X_pred = Et[user_idx].dot(Eb)\n",
        "        if mu is not None:\n",
        "            if isinstance(mu, np.ndarray):\n",
        "                assert mu.size == n_songs  # mu_i\n",
        "                X_pred *= mu\n",
        "            elif isinstance(mu, dict):  # func(mu_ui)\n",
        "                params, func = mu['params'], mu['func']\n",
        "                args = [params[0][user_idx], params[1]]\n",
        "                if len(params) > 2:  # for bias term in document or length-scale\n",
        "                    args += [params[2][user_idx]]\n",
        "                if not callable(func):\n",
        "                    raise TypeError(\"expecting a callable function\")\n",
        "                X_pred *= func(*args)\n",
        "            else:\n",
        "                raise ValueError(\"unsupported mu type\")\n",
        "        X_pred[item_idx] = np.inf\n",
        "        return X_pred\n",
        "\n",
        "    def gen_neg_instances(train_data, U, VT, user_idx, neg_ratio = 1.0, iter = 0):\n",
        "        print ('Job start... %d to %d'%(user_idx.start, user_idx.stop))\n",
        "        #if user_idx.start != 99000: return\n",
        "        batch_users = user_idx.stop - user_idx.start\n",
        "        X_pred = _make_prediction(train_data, U, VT, user_idx, batch_users, vad_data=vad_data)\n",
        "\n",
        "        rows = []\n",
        "        cols = []\n",
        "        total_lost = 0\n",
        "        for idx, uid in enumerate(range(user_idx.start, user_idx.stop)):\n",
        "            num_pos = train_data[uid].count_nonzero()\n",
        "            num_neg = int(num_pos * neg_ratio)\n",
        "            if num_neg <= 0: continue\n",
        "            ranks = X_pred[idx]\n",
        "            neg_withdrawn_prob = compute_neg_prob(ranks)\n",
        "            # print (neg_withdrawn_prob)\n",
        "            neg_instances = list(set(np.random.choice(range(n_items), num_neg, p = neg_withdrawn_prob)))\n",
        "            #rows = rows + len(neg_instances)*[uid]\n",
        "            #uid_dup = np.empty(len(neg_instances))\n",
        "            #uid_dup.fill(uid)\n",
        "            if uid < 0: print ('error with %d to %d'%(user_idx.start, user_idx.stop))\n",
        "            #rows = rows + uid_dup\n",
        "            rows = np.append(rows, np.full( len(neg_instances), uid )  )\n",
        "            cols = np.append(cols, neg_instances)\n",
        "        # print 'check for neg values: ', np.sum(rows <0)\n",
        "        # print 'check for neg values: ', np.sum(cols <0)\n",
        "        if  len(rows) > 0:\n",
        "            path = os.path.join(DATA_DIR, 'sub_dataframe_iter_%d_idxstart_%d.csv' % (iter, user_idx.start))\n",
        "            assert len(rows) == len(cols)\n",
        "            with open(path, 'w') as writer:\n",
        "                for i in range(len(rows)): writer.write(str(rows[i]) + \",\" + str(cols[i]) + '\\n')\n",
        "                writer.flush()\n",
        "            #df = pd.DataFrame({'uid':rows, 'sid':cols}, columns=[\"uid\", \"sid\"], dtype=np.int16)\n",
        "            #df.to_csv(path, sep=\",\",header=False, index = False)\n",
        "        # return df\n",
        "    U, V = None, None\n",
        "\n",
        "    vad_data, vad_raw, vad_df = load_data(os.path.join(DATA_DIR, 'validation.csv'))\n",
        "    train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train.csv'))\n",
        "    test_data, test_raw, test_df = load_data(os.path.join(DATA_DIR, 'test.csv'))\n",
        "    U, V = wmf.decompose(train_data, vad_data, num_factors= n_components)\n",
        "    VT = V.T\n",
        "    iter, max_iter = 0, 10\n",
        "\n",
        "    #load postivie information\n",
        "    X = load_pickle(os.path.join(DATA_DIR, 'item_item_cooc.dat'))\n",
        "    Y = load_pickle(os.path.join(DATA_DIR, 'user_user_cooc.dat'))\n",
        "    X_sppmi = convert_to_SPPMI_matrix(X, max_row=n_items, shifted_K=SHIFTED_K_VALUE)\n",
        "    Y_sppmi = convert_to_SPPMI_matrix(Y, max_row=n_users, shifted_K=SHIFTED_K_VALUE)\n",
        "\n",
        "    best_ndcg100 = 0.0\n",
        "    best_iter = 1\n",
        "    early_stopping = False\n",
        "    while (iter < max_iter and not early_stopping):\n",
        "        ################ Expectation step: ######################\n",
        "        user_slices = rec_eval.user_idx_generator(n_users, batch_users=5000)\n",
        "        print ('GENERATING NEGATIVE INSTANCES ...')\n",
        "        t1 = time.time()\n",
        "        df = Parallel(n_jobs=16)(delayed(gen_neg_instances)(train_data, U, VT, user_idx, neg_ratio = NEGATIVE_SAMPLE_RATIO, iter = iter)\n",
        "                                      for user_idx in user_slices)\n",
        "        t2 = time.time()\n",
        "        print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "\n",
        "        print ('merging to one file ...')\n",
        "        t1 = time.time()\n",
        "        neg_file_out = os.path.join(DATA_DIR, 'train_neg_iter_%d.csv' % (iter))\n",
        "        with open(neg_file_out, 'w') as writer:\n",
        "            writer.write('userId,movieId\\n')\n",
        "        # os.system(\"echo uid,sid >> \" + neg_file_out)\n",
        "        for f in glob.glob(os.path.join(DATA_DIR, 'sub_dataframe_iter*')):\n",
        "            os.system(\"cat \" + f + \" >> \" + neg_file_out)\n",
        "                # with open(f, 'rb') as reader:\n",
        "                #\n",
        "                #     writer.write(reader.readline())\n",
        "            # writer.flush()\n",
        "        #clean\n",
        "        for f in glob.glob(os.path.join(DATA_DIR, 'sub_dataframe_iter*')):\n",
        "            os.remove(f)\n",
        "\n",
        "        t2 = time.time()\n",
        "        print ('Time : %d seconds' % (t2 - t1))\n",
        "        # neg_train_df = pd.concat(df)\n",
        "        # neg_train_df.to_csv(neg_file_out, index = False)\n",
        "        #########################################################\n",
        "\n",
        "        ################ maximization step:######################\n",
        "        print ('GENERATING NEGATIVE EMBEDDINGS ...')\n",
        "        t1 = time.time()\n",
        "        train_neg_data, _, train_neg_df = load_data(neg_file_out, shape=(n_users, n_items))\n",
        "        #build the negative info:\n",
        "        X_neg, _ = pne.produce_neg_embeddings(DATA_DIR, train_neg_data, n_users, n_items, iter = iter)\n",
        "        X_neg_sppmi = convert_to_SPPMI_matrix(X_neg, max_row=n_items, shifted_K=SHIFTED_K_VALUE)\n",
        "        Y_neg_sppmi = None\n",
        "        t2 = time.time()\n",
        "        print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "        # build the model\n",
        "        print ('build the model...')\n",
        "        t1 = time.time()\n",
        "        runner = ModelRunner(train_data, vad_data, None, X_sppmi, X_neg_sppmi, Y_sppmi, None, save_dir=save_dir)\n",
        "        U, V, ndcg100 = runner.run(\"rme\", n_jobs = 1,\n",
        "                                         lam=lam, lam_emb=lam_emb, n_components = n_components, ret_params_only = 1)\n",
        "        t2 = time.time()\n",
        "        print ('Time : %d seconds' % (t2 - t1))\n",
        "        print ('*************************************ITER %d ******************************************' % iter)\n",
        "        print ('NDCG@100 at this iter:',ndcg100)\n",
        "        #\n",
        "        if best_ndcg100 < ndcg100:\n",
        "            best_iter = iter\n",
        "            best_ndcg100 = ndcg100\n",
        "        else:\n",
        "            early_stopping = True\n",
        "        iter += 1\n",
        "        #########################################################\n",
        "    print ('Max NDCG@100: %f , at iter: %d'%(best_ndcg100, best_iter))\n",
        "    best_train_neg_file = os.path.join(DATA_DIR, 'train_neg_iter_%d.csv' % (best_iter))\n",
        "    best_train_neg_file_newname = os.path.join(DATA_DIR, 'train_neg.csv')\n",
        "    best_train_emb_file = os.path.join(DATA_DIR, 'negative_item_item_cooc_iter%d.dat' % (best_iter))\n",
        "    best_train_emb_file_newname = os.path.join(DATA_DIR, 'negative_item_item_cooc.dat')\n",
        "    print ('renaming from %s to %s'%(best_train_neg_file, best_train_neg_file_newname))\n",
        "    os.rename(best_train_neg_file, best_train_neg_file_newname)\n",
        "    print ('renaming from %s to %s' % (best_train_emb_file, best_train_emb_file_newname))\n",
        "    os.rename(best_train_emb_file, best_train_emb_file_newname)\n",
        "    #cleaning\n",
        "    for i in range(max_iter):\n",
        "        if i == best_iter: continue\n",
        "        if early_stopping and (i > best_iter + 1): break\n",
        "        del_file = os.path.join(DATA_DIR, 'train_neg_iter_%d.csv' % ( i))\n",
        "        os.remove(del_file)\n",
        "        del_file = os.path.join(DATA_DIR, 'negative_item_item_cooc_iter%d.dat' % (i))\n",
        "        os.remove(del_file)"
      ],
      "metadata": {
        "id": "3r947Ho9kyZg"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsMENoSYpqcC",
        "outputId": "7f807e0e-dbe9-4694-bae3-2a55d2f05bd1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_NEGATIVE_MATRIX = True\n",
        "if args.model.lower() != 'rme':\n",
        "    LOAD_NEGATIVE_MATRIX = False\n",
        "recalls = np.zeros(5, dtype=np.float32) #store results of topk recommendation in range [5, 10, 20, 50, 100]\n",
        "ndcgs = np.zeros(5, dtype=np.float32)\n",
        "maps = np.zeros(5, dtype=np.float32)\n",
        "print ('*************************************lam =  %.3f ******************************************' % lam)\n",
        "print ('*************************************lam embedding =  %.3f ******************************************' % lam_emb)\n",
        "\n",
        "# train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train_fold%d.csv'%FOLD))\n",
        "vad_data, vad_raw, vad_df = load_data(os.path.join(DATA_DIR, 'validation.csv'))\n",
        "test_data, test_raw, test_df = load_data(os.path.join(DATA_DIR, 'test.csv'))\n",
        "train_data, train_raw, train_df = load_data(os.path.join(DATA_DIR, 'train.csv' ))\n",
        "\n",
        "print ('loading pro_pro_cooc.dat')\n",
        "t1 = time.time()\n",
        "X = load_pickle(os.path.join(DATA_DIR, 'item_item_cooc.dat'))\n",
        "t2 = time.time()\n",
        "print ('[INFO]: sparse matrix size of item item co-occurrence matrix: %d mb\\n' % (\n",
        "    (X.data.nbytes + X.indices.nbytes + X.indptr.nbytes) / (1024 * 1024)))\n",
        "print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "print ('loading user_user_cooc.dat')\n",
        "t1 = time.time()\n",
        "Y = load_pickle(os.path.join(DATA_DIR, 'user_user_cooc.dat'))\n",
        "t2 = time.time()\n",
        "print ('[INFO]: sparse matrix size of user user co-occurrence matrix: %d mb\\n' % (\n",
        "    (Y.data.nbytes + Y.indices.nbytes + Y.indptr.nbytes) / (1024 * 1024)))\n",
        "print ('Time : %d seconds' % (t2 - t1))\n",
        "################# LOADING NEGATIVE CO-OCCURRENCE MATRIX ########################################\n",
        "\n",
        "if LOAD_NEGATIVE_MATRIX:\n",
        "    print ('test loading negative_pro_pro_cooc.dat')\n",
        "    t1 = time.time()\n",
        "    X_neg = load_pickle(os.path.join(DATA_DIR, 'negative_item_item_cooc.dat'))\n",
        "    t2 = time.time()\n",
        "    print ('[INFO]: sparse matrix size of negative item item co-occurrence matrix: %d mb\\n' % (\n",
        "        (X_neg.data.nbytes + X_neg.indices.nbytes + X_neg.indptr.nbytes) / (1024 * 1024)))\n",
        "    print ('Time : %d seconds' % (t2 - t1))\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "########## converting CO-OCCURRENCE MATRIX INTO Shifted Positive Pointwise Mutual Information (SPPMI) matrix ###########\n",
        "####### We already know the user-user co-occurrence matrix Y and item-item co-occurrence matrix X\n",
        "\n",
        "print ('converting co-occurrence matrix into sppmi matrix')\n",
        "t1 = time.time()\n",
        "X_sppmi = convert_to_SPPMI_matrix(X, max_row=n_items, shifted_K=SHIFTED_K_VALUE)\n",
        "Y_sppmi = convert_to_SPPMI_matrix(Y, max_row=n_users, shifted_K=SHIFTED_K_VALUE)\n",
        "t2 = time.time()\n",
        "print ('Time : %d seconds' % (t2 - t1))\n",
        "# if DEBUG_MODE:\n",
        "#     print 'item sppmi matrix'\n",
        "#     print X_sppmi\n",
        "#     print 'user sppmi matrix'\n",
        "#     print Y_sppmi\n",
        "\n",
        "############### Negative SPPMI matrix ##########################\n",
        "X_neg_sppmi = None\n",
        "Y_neg_sppmi = None\n",
        "if LOAD_NEGATIVE_MATRIX:\n",
        "    print ('converting negative co-occurrence matrix into sppmi matrix')\n",
        "    t1 = time.time()\n",
        "    X_neg_sppmi = convert_to_SPPMI_matrix(X_neg, max_row=n_items, shifted_K=SHIFTED_K_VALUE)\n",
        "    t2 = time.time()\n",
        "    print ('Time : %d seconds' % (t2 - t1))\n",
        "################################################################\n",
        "\n",
        "\n",
        "######## Finally, we have train_data, vad_data, test_data,\n",
        "# X_sppmi: item item Shifted Positive Pointwise Mutual Information matrix\n",
        "# Y_sppmi: user-user       Shifted Positive Pointwise Mutual Information matrix\n",
        "\n",
        "\n",
        "print ('Training data', train_data.shape)\n",
        "print ('Validation data', vad_data.shape)\n",
        "print ('Testing data', test_data.shape)\n",
        "\n",
        "n_jobs = 1  # default value\n",
        "model_type = 'model2'  # default value\n",
        "if os.path.exists(save_dir):\n",
        "    #clearning folder\n",
        "    lst = glob.glob(os.path.join(save_dir, '*.*'))\n",
        "    for f in lst:\n",
        "        os.remove(f)\n",
        "else:\n",
        "    os.mkdir(save_dir)\n",
        "\n",
        "\n",
        "runner = ModelRunner(train_data, vad_data, test_data, X_sppmi, X_neg_sppmi, Y_sppmi, Y_neg_sppmi,\n",
        "                       save_dir=save_dir)\n",
        "\n",
        "start = time.time()\n",
        "if args.model == 'wmf':\n",
        "    (recalls, ndcgs, maps) = runner.run(\"wmf\", n_jobs=n_jobs, lam=lam,\n",
        "                                                         saved_model = True,\n",
        "                                                         n_components = n_components)\n",
        "if args.model == 'cofactor':\n",
        "    (recalls, ndcgs, maps) = runner.run(\"cofactor\", n_jobs=n_jobs,\n",
        "                                                        lam=lam,\n",
        "                                                         saved_model=True,\n",
        "                                                         n_components=n_components)\n",
        "if args.model == 'rme':\n",
        "    (recalls, ndcgs, maps) = runner.run(\"rme\", n_jobs=n_jobs,lam=lam, lam_emb = lam_emb,\n",
        "                                                         saved_model=True,\n",
        "                                                         n_components=n_components)\n",
        "end = time.time()\n",
        "print ('total running time: %d seconds'%(end-start))\n",
        "for idx, topk in enumerate([5, 10, 20, 50, 100]):\n",
        "    print ('top-%d results: recall@%d = %.4f, ndcg@%d = %.4f, map@%d = %.4f'%(topk,\n",
        "                                                                                  topk, recalls[idx],\n",
        "                                                                                  topk, ndcgs[idx],\n",
        "                                                                                  topk, maps[idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "KYEzhKNoeNDU",
        "outputId": "5700087a-5c50-4548-99e6-99bb0efa19b7"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************************************lam =  1.000 ******************************************\n",
            "*************************************lam embedding =  1.000 ******************************************\n",
            "loading pro_pro_cooc.dat\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-8ccd7d5aa951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m print ('[INFO]: sparse matrix size of item item co-occurrence matrix: %d mb\\n' % (\n\u001b[0;32m---> 20\u001b[0;31m     (X.data.nbytes + X.indices.nbytes + X.indptr.nbytes) / (1024 * 1024)))\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Time : %d seconds'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load_pickle 했을 때 왜 none값을 갖고 오는지... "
      ],
      "metadata": {
        "id": "BRkUFSAB5KfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU9LrnCprKng",
        "outputId": "66ed2705-0866-4daa-f19d-f970abe31218"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 논문에서는 아래와 같은 결과를 엉었음.\n",
        "\n",
        "\"\"\"\n",
        "top-5 results: recall@5 = 0.1559, ndcg@5 = 0.1613, map@5 = 0.1076\n",
        "top-10 results: recall@10 = 0.1513, ndcg@10 = 0.1547, map@10 = 0.0851\n",
        "top-20 results: recall@20 = 0.1477, ndcg@20 = 0.1473, map@20 = 0.0669\n",
        "top-50 results: recall@50 = 0.1819, ndcg@50 = 0.1553, map@50 = 0.0562\n",
        "top-100 results: recall@100 = 0.2533, ndcg@100 = 0.1825, map@100 = 0.0579\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "J9NiiwjYuN_7",
        "outputId": "08abf395-d09a-43b8-8d74-d5ae71d71e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntop-5 results: recall@5 = 0.1559, ndcg@5 = 0.1613, map@5 = 0.1076\\ntop-10 results: recall@10 = 0.1513, ndcg@10 = 0.1547, map@10 = 0.0851\\ntop-20 results: recall@20 = 0.1477, ndcg@20 = 0.1473, map@20 = 0.0669\\ntop-50 results: recall@50 = 0.1819, ndcg@50 = 0.1553, map@50 = 0.0562\\ntop-100 results: recall@100 = 0.2533, ndcg@100 = 0.1825, map@100 = 0.0579\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMYibry3QICF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}